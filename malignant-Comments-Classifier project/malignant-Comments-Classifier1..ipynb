{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train=pd.read_csv('train123.csv')\n",
    "comments_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_test=pd.read_csv('testM.csv', encoding=('ISO-8859-1'),low_memory=False)\n",
    "comments_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeatures Present in the Dataset: \\n\", comments_train.columns)\n",
    "shape=comments_train.shape\n",
    "print(\"\\nTotal Number of Rows : \",shape[0])\n",
    "print(\"Total Number of Features : \", shape[1])\n",
    "print(\"\\n\\nData Types of Features :\\n\", comments_train.dtypes)\n",
    "print(\"\\nDataset contains any NaN/Empty cells : \", comments_train.isnull().values.any())\n",
    "print(\"\\nTotal number of empty rows in each feature:\\n\", comments_train.isnull().sum(),\"\\n\\n\")\n",
    "print(\"Total number of unique values in each feature:\")\n",
    "for col in comments_train.columns.values:\n",
    "    print(\"Number of unique values of {} : {}\".format(col, comments_train[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['malignant', 'highly_malignant', 'rude', 'threat','abuse', 'loathe',]\n",
    "for col in cols:\n",
    "    print(\"Number of value_counts of {} : {}\".format(col, comments_train[col].nunique()))\n",
    "    print(comments_train[f'{col}'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train.drop(['id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = comments_train.comment_text.str.len()\n",
    "lens.mean(), lens.std(), lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train.corr().style.background_gradient(cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [ 'malignant', 'highly_malignant', 'rude', 'threat','abuse', 'loathe']\n",
    "comments_train['label'] = 1-comments_train[label_cols].max(axis=1)\n",
    "comments_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train['length'] = comments_train.comment_text.str.len()\n",
    "comments_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Here I have made a function in which all the Data cleaning steps like removing data which is not useful like \n",
    "#       email adress, mobile numbers,removing punctuations, converting all the documents into lowercase, \n",
    "#           using lemmatization technique, filtering documents using Stopwords, using POS tagging,\n",
    "#      all these type of data preprocessing steps are being perormed with th ehelp of the function defined below.\n",
    "\n",
    "# function to filter using POS tagging..\n",
    "def get_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function for data cleaning...\n",
    "def Processed_data(comments):\n",
    "    # Replace email addresses with 'email'\n",
    "    comments=re.sub(r'^.+@[^\\.].*\\.[a-z]{2,}$',' ', comments)\n",
    "        \n",
    "    # Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "    comments=re.sub(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',' ',comments)\n",
    "        \n",
    "    # getting only words(i.e removing all the special characters)\n",
    "    comments = re.sub(r'[^\\w]', ' ', comments) \n",
    "        \n",
    "    # getting only words(i.e removing all the\" _ \")\n",
    "    comments = re.sub(r'[\\_]', ' ', comments) \n",
    "    \n",
    "    # getting rid of unwanted characters(i.e remove all the single characters left)\n",
    "    comments=re.sub(r'\\s+[a-zA-Z]\\s+', ' ', comments)\n",
    "    \n",
    "    # Removing extra whitespaces\n",
    "    comments=re.sub(r'\\s+', ' ', comments, flags=re.I)\n",
    "\n",
    "    #converting all the letters of the review into lowercase\n",
    "    comments = comments.lower()\n",
    "    \n",
    "    # splitting every words from the sentences\n",
    "    comments = comments.split()\n",
    "\n",
    "    # iterating through each words and checking if they are stopwords or not,\n",
    "    comments=[word for word in comments if not word in set(STOPWORDS)]\n",
    "    \n",
    "    # remove empty tokens\n",
    "    comments = [text for text in comments if len(text) > 0]\n",
    "    \n",
    "    # getting pos tag text\n",
    "    pos_tags = pos_tag(comments)\n",
    "\n",
    "    # considering words having length more than 3only\n",
    "    comments = [text for text in comments if len(text) > 3]        \n",
    "   \n",
    "    # performing lemmatization operation and passing the word in get_pos function to get filtered using POS ... \n",
    "    comments = [(WordNetLemmatizer().lemmatize(text[0], get_pos(text[1])))for text in pos_tags]\n",
    "\n",
    "   # considering words having length more than 3 only\n",
    "    comments = [text for text in comments if len(text) > 3]\n",
    "    comments = ' '.join(comments)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train[\"clean_comment_text\"] = comments_train[\"comment_text\"].apply(lambda x: Processed_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train[\"clean_comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train['Clean_length'] = comments_train.clean_comment_text.str.len()\n",
    "comments_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train[comments_train['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train[comments_train['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=comments_train.iloc[:,2:8].sum()\n",
    "plt.figure(figsize=(10,6))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Malignant and Non Malignant comments Counts\")\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Comments Type ', fontsize=12)\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "from nltk.tokenize import word_tokenize\n",
    "for j,i in enumerate(comments_train['clean_comment_text']):\n",
    "    a=word_tokenize(i,'english')\n",
    "    data.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = comments_train['label'].value_counts().plot(kind='barh', figsize=(15,7),color=\"coral\", fontsize=13)\n",
    "ax.set_alpha(0.8)\n",
    "\n",
    "ax.set_title(\"Malignant Vs Not Malignant Comments\", fontsize=18)\n",
    "ax.set_xlabel(\"Counts\",  fontsize=18)\n",
    "ax.set_ylabel(\"'1: Not Malignant'            '0:  Malignant'\", fontsize=18)\n",
    "totals = []\n",
    "\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_width())\n",
    "\n",
    "total = sum(totals)\n",
    "\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width()+.3, i.get_y()+.38, str(round((i.get_width()/total)*100, 2))+'%', fontsize=15,color='dimgrey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 = Counter(\" \".join(comments_train[comments_train['label']==0][\"clean_comment_text\"]).split()).most_common(200)\n",
    "count2 = Counter(\" \".join(comments_train[comments_train['label']==1][\"clean_comment_text\"]).split()).most_common(200)\n",
    "df=pd.DataFrame()\n",
    "print(\"TOP 200 Words in Each Category (Word,Counts)\")\n",
    "df['Malignant Words']=count1\n",
    "df['Not Malignant Words']=count2\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Display_wordcloud(data,title):\n",
    "    feedbackcloud = WordCloud(\n",
    "        background_color = 'black',\n",
    "        max_words = 1000,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 25\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (15, 10),facecolor='g')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(feedbackcloud)\n",
    "    plt.title(f\"{title} words\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_wordcloud(comments_train['clean_comment_text'][comments_train['label']==1],\"NOT MALIGNANT WORDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_wordcloud(comments_train['clean_comment_text'][comments_train['label']==0],\"MALIGNANT WORDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize = (15,8))\n",
    "\n",
    "sns.distplot(comments_train[comments_train['label']==0]['length'],bins=20,ax=ax[0],label='MALIGNANT words distribution',color='g')\n",
    "\n",
    "ax[0].set_xlabel('MALIGNANT words length')\n",
    "ax[0].legend()\n",
    "\n",
    "sns.distplot(comments_train[comments_train['label']==1]['length'],bins=20,ax=ax[1],label='Not MALIGNANT wordsdistribution')\n",
    "ax[1].set_xlabel('Not MALIGNANT words length')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize = (15,8))\n",
    "\n",
    "sns.distplot(comments_train[comments_train['label']==0]['Clean_length'],bins=20,ax=ax[0],label='MALIGNANT words distribution',color='g')\n",
    "\n",
    "ax[0].set_xlabel('MALIGNANT words length')\n",
    "ax[0].legend()\n",
    "\n",
    "sns.distplot(comments_train[comments_train['label']==1]['Clean_length'],bins=20,ax=ax[1],label='Not MALIGNANT words distribution')\n",
    "ax[1].set_xlabel('Not MALIGNANT words length')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tf_idf_train(text):\n",
    "    tfid = TfidfVectorizer(min_df=3,smooth_idf=False)\n",
    "    return tfid.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Tf_idf_train(comments_train['clean_comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x: \",x.shape)\n",
    "y = comments_train['label'].values\n",
    "print(\"Shape of y: \",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression,PassiveAggressiveClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "LR=LogisticRegression()\n",
    "MNB=MultinomialNB()\n",
    "PAC=PassiveAggressiveClassifier()\n",
    "DT=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "models.append(('LogisticRegression',LR))\n",
    "models.append(('MultinomialNB',MNB))\n",
    "models.append(('PassiveAggressiveClassifier',PAC))\n",
    "models.append(('DecisionTreeClassifier',DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_acc_score(clf,x,y):\n",
    "    max_acc_score=0\n",
    "    final_r_state=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.30,random_state=r_state,stratify=y)\n",
    "        clf.fit(x_train,y_train)\n",
    "        y_pred=clf.predict(x_test)\n",
    "        acc_score=accuracy_score(y_test,y_pred)\n",
    "        if acc_score > max_acc_score:\n",
    "            max_acc_score=acc_score\n",
    "            final_r_state=r_state\n",
    "    print('Max Accuracy Score corresponding to Random State ', final_r_state, 'is:', max_acc_score)\n",
    "    print('\\n')\n",
    "    return final_r_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Model=[]\n",
    "Score=[]\n",
    "Acc_score=[]\n",
    "cvs=[]\n",
    "rocscore=[]\n",
    "lg_loss=[]\n",
    "\n",
    "for name,model in models:\n",
    "    print('***************************',name,'*****************************')\n",
    "    print('\\n')\n",
    "    Model.append(name)\n",
    "    print(model)\n",
    "    print('\\n')\n",
    "            \n",
    "    r_state=max_acc_score(model,x,y)\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=r_state,stratify=y)\n",
    "    model.fit(x_train,y_train)\n",
    "    score=model.score(x_train,y_train)\n",
    "    print('Learning Score : ',score)\n",
    "    Score.append(score*100)\n",
    "    y_pred=model.predict(x_test)\n",
    "    acc_score=accuracy_score(y_test,y_pred)\n",
    "    print('Accuracy Score : ',acc_score)\n",
    "    Acc_score.append(acc_score*100)\n",
    "       \n",
    "    cv_score=cross_val_score(model,x,y,cv=10,scoring='roc_auc').mean()\n",
    "    print('Cross Val Score : ', cv_score)\n",
    "    cvs.append(cv_score*100)\n",
    "        \n",
    "    false_positive_rate,true_positive_rate, thresholds=roc_curve(y_test,y_pred)\n",
    "    roc_auc=auc(false_positive_rate, true_positive_rate)\n",
    "    print('roc auc score : ', roc_auc)\n",
    "    rocscore.append(roc_auc*100)\n",
    "    print('\\n')  \n",
    "    \n",
    "    loss = log_loss(y_test,y_pred)\n",
    "    print('Log loss : ', loss)\n",
    "    lg_loss.append(loss)\n",
    "    print('\\n')\n",
    "      \n",
    "    print('Classification Report:\\n',classification_report(y_test,y_pred))\n",
    "    print('\\n')\n",
    "    \n",
    "    print('Confusion Matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10,40))\n",
    "    plt.subplot(911)\n",
    "    plt.title(name)\n",
    "    plt.plot(false_positive_rate,true_positive_rate,label='AUC = %0.2f'% roc_auc)\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('True_positive_rate')\n",
    "    plt.xlabel('False_positive_rate')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({'Model': Model,'Learning Score': Score,'Accuracy Score': Acc_score,'Cross Val Score':cvs,\n",
    "                     'Roc_Auc_curve':rocscore,'Log_Loss':lg_loss}) \n",
    "result.style.background_gradient(cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=Model,x=Acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=57,test_size=.30,stratify=y)\n",
    "PAC=PassiveAggressiveClassifier()\n",
    "PAC.fit(x_train,y_train)\n",
    "PAC.score(x_train,y_train)\n",
    "PACpred=PAC.predict(x_test)\n",
    "print('Accuracy Score:',accuracy_score(y_test,PACpred))\n",
    "print('Log loss : ', log_loss(y_test,PACpred))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test,PACpred))\n",
    "print('Classification Report:','\\n',classification_report(y_test,PACpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(3,3))\n",
    "sns.heatmap(confusion_matrix(y_test, PACpred),annot=True,linewidths=1,center=0,cmap='YlGnBu')\n",
    "plt.xlabel(\"True label\")\n",
    "plt.ylabel(\"Predicted label\")\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize = (15,6))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, PACpred)\n",
    "ax.plot([0,1],[0,1],'r--')\n",
    "ax.plot(fpr,tpr,label='AUC = %0.2f'% roc_auc_score(y_test, PACpred))\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlabel('false positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.set_title('PassiveAggressiveClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tf_idf_test(text):\n",
    "    tfid = TfidfVectorizer(max_features=43194,smooth_idf=False)\n",
    "    return tfid.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_test[\"clean_comment_text\"] = comments_test[\"comment_text\"].apply(lambda x: Processed_data(x))\n",
    "x_testing_data=Tf_idf_test(comments_test['clean_comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction=PAC.predict(x_testing_data)\n",
    "testM['Predicted values']=Prediction\n",
    "testM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testM.to_csv('Malignant_Comment_Classifier_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(PAC,'Malignant_Comment_Classifier_Predict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
